{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51953757-7d6b-48f2-acbb-3d6606b293c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-14 22:47:27--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/refs/heads/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4073 (4.0K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.98K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-14 22:47:27 (26.2 MB/s) - ‘minsearch.py’ saved [4073/4073]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/refs/heads/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c62e2a6-e683-4c4d-a668-3626c8f2d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minsearch\n",
      "  Downloading minsearch-0.0.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from minsearch) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from minsearch) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->minsearch) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->minsearch) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
      "Downloading minsearch-0.0.2-py3-none-any.whl (4.1 kB)\n",
      "Installing collected packages: minsearch\n",
      "Successfully installed minsearch-0.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import minsearch\n",
    "!pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3654c5e-0671-45f6-bcdb-e47790b64006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68253b82-5781-475a-9211-e01af066bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/refs/heads/main/01-intro/documents-llm.json\"\n",
    "\n",
    "response = requests.get(data_url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fe6912-5987-476a-be6f-39b71aa06928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'llm-zoomcamp',\n",
       "  'documents': [{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I just discovered the course. Can I still join?'},\n",
       "   {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?'},\n",
       "   {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?'},\n",
       "   {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How do I get access?'},\n",
       "   {'text': 'We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - How many free hours do we get?'},\n",
       "   {'text': 'This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month'},\n",
       "   {'text': 'Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Cloud alternatives with GPU'},\n",
       "   {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "   {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "   {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I missed the first homework - can I still get a certificate?'},\n",
       "   {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'I was working on next week’s homework/content - why does it keep changing?'},\n",
       "   {'text': 'Summer 2025 (via Alexey).',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'When will the course be offered next?'},\n",
       "   {'text': 'Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.',\n",
       "    'section': 'General course-related questions',\n",
       "    'question': 'Are there any lectures/videos? Where are they?'},\n",
       "   {'text': 'Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.'},\n",
       "   {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error when running OpenAI chat.completions.create command'},\n",
       "   {'text': \"RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Error: RateLimitError: Error code: 429 -'},\n",
       "   {'text': 'Update openai version from 0.27.0 -> any 1.x version',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\"},\n",
       "   {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: How much will I have to spend to use the Open AI API?'},\n",
       "   {'text': \"No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?'},\n",
       "   {'text': 'If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'ElasticSearch: ERROR: Elasticsearch exited unexpectedly'},\n",
       "   {'text': 'Instead of document as used in the course video, use doc',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Docker: How do I store data persistently in Elasticsearch?'},\n",
       "   {'text': \"You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Safe and easy way to store and load API keys'},\n",
       "   {'text': 'Option1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?'},\n",
       "   {'text': 'Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: How can I use Ollama open-source models locally on my pc without using any API?'},\n",
       "   {'text': \"The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\",\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': \"OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\"},\n",
       "   {'text': 'You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use Groq instead of OpenAI?'},\n",
       "   {'text': 'Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'OpenSource: Can I use open-source alternatives to OpenAI API?'},\n",
       "   {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.',\n",
       "    'section': 'Module 1: Introduction',\n",
       "    'question': 'Returning Empty list after filtering my query (HW Q3)'},\n",
       "   {'text': 'Answer',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Question'},\n",
       "   {'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Saturn Cloud issues'},\n",
       "   {'text': 'Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?'},\n",
       "   {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?'},\n",
       "   {'text': 'Yes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?'},\n",
       "   {'text': 'You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'ElasticSearch: How can I limit the memory used by the ElasticSearch container?'},\n",
       "   {'text': 'You have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How to inspect the content of a file inside a Docker container ?'},\n",
       "   {'text': 'Use the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Error: Docker mounted volume adds ;C to end of windows path'},\n",
       "   {'text': 'In Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?'},\n",
       "   {'text': 'docker system prune -a',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Docker: How can to clean docker cache?'},\n",
       "   {'text': 'A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama'},\n",
       "   {'text': 'To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …'},\n",
       "   {'text': 'ollama list\\nollama rm [model_name]',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: How can remove Ollama model?'},\n",
       "   {'text': 'InternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Ollama: Error code 500 InternalServerError'},\n",
       "   {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF'},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': \"Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\"},\n",
       "   {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3'},\n",
       "   {'text': 'pip install protobuf==3.20.1\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'Python: from google.protobuf.pyext import _message / TypeError: bases must be types'},\n",
       "   {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal',\n",
       "    'section': 'Module 2: Open-Source LLMs',\n",
       "    'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?'},\n",
       "   {'text': 'The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'How to run a model using CUDA for GPU usage?'},\n",
       "   {'text': 'Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\"},\n",
       "   {'text': 'This worked for me:',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\"},\n",
       "   {'text': 'Try to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search'},\n",
       "   {'text': 'As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?'},\n",
       "   {'text': 'For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Not module named “sentence_transformers”'},\n",
       "   {'text': 'I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Can not create the index: Connection timeout.'},\n",
       "   {'text': 'Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': \"TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\"},\n",
       "   {'text': 'max_value = numpy_array.max()',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'Find maximum of an numpy array (of any dimension):'},\n",
       "   {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What is the cosine similarity?'},\n",
       "   {'text': 'A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.',\n",
       "    'section': 'Module 3: X',\n",
       "    'question': 'What are documents in ElasticSearch?'},\n",
       "   {'text': 'docker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.'},\n",
       "   {'text': 'Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'Warning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?'},\n",
       "   {'text': 'Solution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.'},\n",
       "   {'text': 'Inside .env file change POSTGRES_HOST=localhost',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?'},\n",
       "   {'text': 'By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook'},\n",
       "   {'text': 'import numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': 'To compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)',\n",
       "    'section': 'Module 4: Monitoring',\n",
       "    'question': 'How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?'},\n",
       "   {'text': '1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': 'How can I remove all Docker containers, images, and volumes, and builds from the terminal?'},\n",
       "   {'text': 'Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>',\n",
       "    'section': 'Module 5: X',\n",
       "    'question': \"I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\"},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Module 6: X', 'question': 'Question'},\n",
       "   {'text': 'Answer', 'section': 'Capstone Project', 'question': 'Question'},\n",
       "   {'text': 'No, the capstone is a solo project.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Is it a group project?'},\n",
       "   {'text': 'You only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Do we submit 2 projects, what does attempt 1 and 2 mean?'},\n",
       "   {'text': 'No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'Does the competition count as the capstone?'},\n",
       "   {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).',\n",
       "    'section': 'Capstone Project',\n",
       "    'question': 'How is my capstone project going to be evaluated?'},\n",
       "   {'text': 'Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.',\n",
       "    'section': 'Certificates',\n",
       "    'question': 'Do I have to use ElasticSearch or X library?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: dlthub', 'question': 'Question'},\n",
       "   {'text': 'Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Can I use the workshop materials for my own projects or share them with others?'},\n",
       "   {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)'},\n",
       "   {'text': 'You can use the db.table_names() to list all the tables in the db',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'How do I know which tables are in the db'},\n",
       "   {'text': 'Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Does DLT have connectors to ClickHouse or StarRocks?'},\n",
       "   {'text': 'If you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.',\n",
       "    'section': 'Workshops: dlthub',\n",
       "    'question': 'Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search'},\n",
       "   {'text': 'Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?'},\n",
       "   {'text': 'If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi',\n",
       "    'section': 'Workshops: X',\n",
       "    'question': 'Connection refused error on prompting the ollam RAG?'},\n",
       "   {'text': 'Answer', 'section': 'Workshops: X', 'question': 'Question'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b744bae-d872-4dc2-9a6c-8022903e3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for course_dict in data:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a361a4-57c9-4985-a1f7-086cf0a15f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I just discovered the course. Can I still join?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'SaturnCloud - How do I get access?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'SaturnCloud - How many free hours do we get?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\\nGoogle Colab\\nKaggle\\nDatabricks (?), so many others.\\nUse GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Cloud alternatives with GPU',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I missed the first homework - can I still get a certificate?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I was working on next week’s homework/content - why does it keep changing?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Summer 2025 (via Alexey).',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'When will the course be offered next?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Are there any lectures/videos? Where are they?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\\\Users\\\\YourUsername\\\\.wslconfig) with the desired RAM allocation:\\n[wsl2]\\nmemory=8GB\\nRestart WSL: wsl --shutdown\\nRun the free command to verify the changes. For more details, read this article.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenAI: Error when running OpenAI chat.completions.create command',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\\nThe above errors are related to your OpenAI API account’s quota.\\nThere is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\\nSteps to resolve:\\nAdd credits to your account here (min $5)\\nIn chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\\nYou might need to recreate an API key after adding credits to your account and update it locally.\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenAI: Error: RateLimitError: Error code: 429 -',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Update openai version from 0.27.0 -> any 1.x version',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': \"OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenAI: How much will I have to spend to use the Open AI API?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\\nllm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\\nI specified the RAM size to the configuration (-m 4GB)\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-m 4GB \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nOr give it _less_ RAM:\\nTip for Github Codespace users\\nIf you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\\nIn that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\\nThis reduces the resource usage.\\nFull command:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nIf it doesn\\'t work, try this:\\nsudo sysctl -w vm.max_map_count=262144\\nAnd give the Java machine inside the container more RAM:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n--ulimit nofile=65536:65536 \\\\\\n--ulimit memlock=-1:-1 \\\\\\n--memory=4g \\\\\\n--cpus=2 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnother possible solution may be to set the memory_lock to false:\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\n-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\\\\n-e \"bootstrap.memory_lock=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'ElasticSearch: ERROR: Elasticsearch exited unexpectedly',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Instead of document as used in the course video, use doc',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': \"ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\\ndocker volume create elasticsearch_data\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-v elasticsearch_data:/usr/share/elasticsearch/data \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Docker: How do I store data persistently in Elasticsearch?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\\nFor example, you can create a new file named “api_keys.yml” in your repository.\\nThen, do not forget to add it in your .gitignore file:\\n#api_keys\\napi_keys.yml\\nYou can now fill your api_keys.yml file:\\nOPENAI_API_KEY: “sk[...]”\\nGROQ_API_KEY: “gqk_[...]”\\nSave your file.\\nYou will need the pyyaml library to load your yaml file, so run this command in your terminal:\\npip install pyyaml\\nNow, open your jupyter notebook.\\nYou can load your yaml file and the associated keys with this code:\\nimport yaml\\n# Open the file\\nwith open('api_keys.yml', 'r') as file:\\n# Load the data from the file\\ndata = yaml.safe_load(file)\\n# Get the API key (Groq example here)\\ngroq_api_key = data['GROQ_API_KEY']\\nNow, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\\nAdded by Mélanie Fouesnard\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Authentication: Safe and easy way to store and load API keys',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Option1: using direnv\\ncreated the .envrc file & added my API key, ran direnv allow in the terminal\\nwas getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\\nresolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\\nfrom dotenv import load_dotenv\\nload_dotenv(\\'.envrc\\')\\nOption 2: using Codespaces Secrets\\nLog in to your GitHub account and navigate to Settings > Codespaces\\nThere is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\\nOnce you set this up, the key will be available in your codespaces session',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\\nTo download ollama for Ubuntu:\\n``` curl -fsSL https://ollama.com/install.sh | sh ```\\nTo download ollama for Mac and Windows, follow the guide on this link:\\nhttps://ollama.com/download/\\nOllama a number of open-source LLMs like:\\nLlama3\\nPhi3\\nMistral and Mixtral\\nGemma\\nQwen\\nYou can explore more models on https://ollama.com/library/\\nTo download a model in Ollama, simply open command prompt and type:\\n``` ollama run model_name ```\\ne.g.\\n``` ollama run phi3 ```\\nIt will automatically download the model and you can use it same way as above for later time.\\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\\nimport ollama\\ndef llm(prompt):\\nresponse = ollama.chat(\\nmodel=\"llama3\",\\nmessages=[{\"role\": \"user\", \"content\": prompt}]\\n)\\nreturn response[\\'message\\'][\\'content\\']\\nFor example, we can use it in the following way:\\nprompt = \"When does the llm-zoomcamp course start?\"\\nanswer = llm(prompt)\\nprint(answer)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenSource: How can I use Ollama open-source models locally on my pc without using any API?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': \"The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': \"OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenSource: Can I use Groq instead of OpenAI?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenSource: Can I use open-source alternatives to OpenAI API?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Returning Empty list after filtering my query (HW Q3)',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Saturn Cloud issues',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\\nOnce you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\\n1- Navigate to Your Project Directory:\\ncd /home/jovyan/my_project\\n2- Configure GitHub Remote to Use SSH:\\ngit remote set-url origin git@github.com:username/repository.git\\n3- Stage, Commit and push your changes:\\ngit add .\\ngit commit -m \"Your commit message\"\\ngit push',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Yes, you can. Here the step to follow:\\n- Open a bash session in the elasticsearch container\\n```bash\\ndocker exec -it elasticsearch bash\\n```\\n- Add path.repo configuration:\\n```bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart container and verify it was created correctly:\\n```bash\\ndocker restart elasticsearch\\ncurl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\\n```\\n- Create the snapshot (this is the backup ;) )\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Copy the backup to my machine:\\n```bash\\ndocker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\\n```\\n- Now create the new container or use docker-compose just in case you are following the module 2:\\n```bash\\ndocker compose up -d\\n```\\n- Add de path.repo configuration in the new one, same as before:\\n```bash\\ndocker exec -it new_elasticsearch bash\\necho path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\\n```\\n- Restart the docker container and copy the snapshot in it:\\n```bash\\ndocker restart new_elasticsearch\\ndocker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\\n```\\n- Register the Snapshot Repository in the New Container:\\n```bash\\ncurl -X PUT \"localhost:9200/_snapshot/my_backup\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"type\": \"fs\",\\n\"settings\": {\\n\"location\": \"/usr/share/elasticsearch/backup\"\\n}\\n}\\n\\'\\n```\\n- Verify if it exists:\\n```bash\\ncurl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\\n```\\n- Restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"your_index_name\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false\\n}\\n\\'\\n```\\n- Show your indexes:\\n```bash\\ncurl -X GET \"localhost:9200/_cat/indices?v\"\\n```\\n- Extra point: If you want to change the original index name by other when you restore the snapshot:\\n```bash\\ncurl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n\"indices\": \"old_index\",\\n\"ignore_unavailable\": true,\\n\"include_global_state\": false,\\n\"rename_pattern\": \"old_index\",\\n\"rename_replacement\": \"new_index\"\\n}\\n\\'\\n```',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\\n- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\\n- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\ncpus: \\'1.0\\'  # Limits to 1 CPU\\nreservations:\\ncpus: \\'0.5\\'  # Reserves 0.5 CPUs',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'ElasticSearch: How can I limit the memory used by the ElasticSearch container?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You have several ways to inspect the content of a file when you are inside a Docker container.\\nFirst, make sure you ran the docker container interactively using bash:\\ndocker exec -it <container> bash\\nThen, you are able to use bash commands. For this case, I propose two solutions:\\nUse “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\\nInstall vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\\napt-get install vim\\nvim your_file\\nThen, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\\nAdded by Mélanie Fouesnard',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Docker: How to inspect the content of a file inside a Docker container ?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Use the following line instead in mounting the current volume to docker for Q4:\\n`-v \"/${PWD}/ollama_files:/root/.ollama\"`',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Docker: Error: Docker mounted volume adds ;C to end of windows path',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'In Docker Desktop, try to increase the resource.\\nGo to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\\nAdded by Dandy Arif Rahman',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'docker system prune -a',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Docker: How can to clean docker cache?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\\nAdded by Eduardo Muñoz',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\\nAdded by Taras Goriachko\\nOllama: Running Ollama locally on Colab gives error after the llm() line\\nAPIConnectionError: Connection error.\\nIt seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\\nFound a solution in the Medium article and this link:\\nhttps://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\\nhttps://github.com/ollama/ollama/issues/703\\nAdded by Hanaa',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Ollama: Error: NotFoundError: Error code: 404 - {\\'error\\': {\\'message\\': \"model XXX not found, try pulling it first\" …',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'ollama list\\nollama rm [model_name]',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Ollama: How can remove Ollama model?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'InternalServerError: Error code: 500 - {\\'error\\': {\\'message\\': \\'model requires more system memory (5.6 GiB) than is available (1.5 GiB)\\', \\'type\\': \\'api_error\\', \\'param\\': None, \\'code\\': None}}.\\nRunning elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\\nversion: \\'3.8\\'\\nservices:\\nelasticsearch:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\\ncontainer_name: elasticsearch\\nenvironment:\\n- discovery.type=single-node\\n- xpack.security.enabled=false\\n- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\\nports:\\n- \"9200:9200\"\\n- \"9300:9300\"\\ndeploy:\\nresources:\\nlimits:\\nmemory: 2G  # change 2\\nollama:\\nimage: ollama/ollama\\ncontainer_name: ollama\\nvolumes:\\n- ollama:/root/.ollama\\nports:\\n- \"11434:11434\"\\nvolumes:\\nollama:\\nAdded by Zoe Zelkha',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Ollama: Error code 500 InternalServerError',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': \"Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'To solve just install transformers directly from github\\n!pip install git+https://github.com/huggingface/transformers',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'pip install protobuf==3.20.1\\nAdded by Ibai Irastorza',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Python: from google.protobuf.pyext import _message / TypeError: bases must be types',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon’t forget to make your token secret.\\nAdded by kamal',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\\nIn my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\\nTo install all the needed packages use this command:\\n!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\\nAnd after that just executed this command:\\n!pip install --upgrade transformers',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'How to run a model using CUDA for GPU usage?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\\nOr use the keyword ‘body’ instead of ‘document’\\nFor conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\\n\\n$ conda config --add channels conda-forge\\n$ conda config --set channel_priority strict\\n$ conda install -c conda-forge elasticsearch==8.4.3',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': \"ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'This worked for me:',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': \"ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Try to running docker container based on first course module like this :\\ndocker run -it \\\\\\n--rm \\\\\\n--name elasticsearch \\\\\\n-p 9200:9200 \\\\\\n-p 9300:9300 \\\\\\n-e \"discovery.type=single-node\" \\\\\\n-e \"xpack.security.enabled=false\" \\\\\\ndocker.elastic.co/elasticsearch/elasticsearch:8.4.3\\nAnd don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\\nThe KNN related score, which is between -1 and 1 (cosine similarity)\\nThe text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document\\'s relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\\nSince we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\\nTo get more details about the final scores, you can modify the search query and add an “explain” parameter:\\nresponse = es_client.search(\\nindex=index_name,\\nquery={\\n\"match\": {\"section\": \"General course-related questions\"},\\n},\\nknn=knn_query,\\nsize=5,\\nexplain=True\\n)\\nAdded by Mélanie Fouesnard',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\\npip install sentence-transformers',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'Not module named “sentence_transformers”',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\\nI checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\\nMy problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\\ndocker image prune\\ndocker volume prune\\ndocker container prune\\nAdded by Ibai Irastorza',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'Can not create the index: Connection timeout.',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\\nv_query = embedding_model.encode(query_text)\\nresults = search_function(v_query)',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': \"TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'max_value = numpy_array.max()',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'Find maximum of an numpy array (of any dimension):',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'What is the cosine similarity?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.',\n",
       "  'section': 'Module 3: X',\n",
       "  'question': 'What are documents in ElasticSearch?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'docker stop elasticsearch\\ndocker rm elasticsearch\\nHow to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\\nscore = (es_score - 0.5) * 2',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See \\'docker run --help\\'.',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'Warning: \\'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet\\' how to suppress?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Solution 1 : Install Visual C++ Redistributable\\nSolution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\fbgemm.dll\" or one of its dependencies.',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Inside .env file change POSTGRES_HOST=localhost',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\\nThe instruction to show the results must be preceded by:\\npd.set_option(\\'display.max_colwidth\\', None)\\nHere are the specs for the display_max_colwidth option, as describide in the official docs:\\ndisplay.max_colwidth : int or None\\nThe maximum width in characters of a column in the repr of\\na pandas data structure. When the column overflows, a \"...\"\\nplaceholder is embedded in the output. A \\'None\\' value means unlimited.\\n[default: 50] [currently: 50]',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'import numpy as np\\nnormalize_vec = lambda v: v / np.linalg.norm(v)\\ndf[\"new_col\"] = df[\"org_col\"].apply(norm_vec)',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'To compute the 75% percentile or 0.75 quantile:\\nquantile: int = df[\"col\"].quantile(q=0.75)',\n",
       "  'section': 'Module 4: Monitoring',\n",
       "  'question': 'How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': '1. Delete all containers (including running ones):\\n```\\ndocker rm -f\\n```\\n2. Remove all images:\\n```\\ndocker rmi -f\\n```\\n3. Delete all volumes:\\n```\\ndocker volume rm\\n```',\n",
       "  'section': 'Module 5: X',\n",
       "  'question': 'How can I remove all Docker containers, images, and volumes, and builds from the terminal?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>',\n",
       "  'section': 'Module 5: X',\n",
       "  'question': \"I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\",\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Module 6: X',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Module 6: X',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Capstone Project',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'No, the capstone is a solo project.',\n",
       "  'section': 'Capstone Project',\n",
       "  'question': 'Is it a group project?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You only need to submit 1 project. \\nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\\nIf you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort',\n",
       "  'section': 'Capstone Project',\n",
       "  'question': 'Do we submit 2 projects, what does attempt 1 and 2 mean?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.',\n",
       "  'section': 'Capstone Project',\n",
       "  'question': 'Does the competition count as the capstone?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).',\n",
       "  'section': 'Capstone Project',\n",
       "  'question': 'How is my capstone project going to be evaluated?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.',\n",
       "  'section': 'Certificates',\n",
       "  'question': 'Do I have to use ElasticSearch or X library?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\\nAdded by Mélanie Fouesnard',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'Can I use the workshop materials for my own projects or share them with others?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'You can use the db.table_names() to list all the tables in the db',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'How do I know which tables are in the db',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'Does DLT have connectors to ClickHouse or StarRocks?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'If you get this error\\nOr 401 Client Error , then you either need to grant access to the key or the key is wrong.',\n",
       "  'section': 'Workshops: dlthub',\n",
       "  'question': 'Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`',\n",
       "  'section': 'Workshops: X',\n",
       "  'question': 'Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\\nIf you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\\nAdded by Abiodun Gbadamosi',\n",
       "  'section': 'Workshops: X',\n",
       "  'question': 'Connection refused error on prompting the ollam RAG?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Answer',\n",
       "  'section': 'Workshops: X',\n",
       "  'question': 'Question',\n",
       "  'course': 'llm-zoomcamp'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a543e783-e9a8-47dc-96d0-5968fdfcce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-zoomcamp/minsearch.py:10: UserWarning: Now minsearch is installable via pip: 'pip install minsearch'. Remove the downloaded file and re-install it with pip.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "index = Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e089ce-cbf9-4977-a204-fff8ce021e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x78cfdee6bc80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e44f23-13bb-490d-bd94-f1bebde6d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cd2cad-d1b1-4936-9a76-7e9b0b5dceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question':3.0, 'section':0.5}\n",
    "results = index.search(\n",
    "    query='When does the course start?',\n",
    "    #If there is more than one course : filter_dict={'course':'data-engineering-zoomcamp'}, \n",
    "    boost_dict=boost,\n",
    "    num_results=1\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae75f9d-a10a-40de-b931-87c01680ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Summer 2025 (via Alexey).', 'section': 'General course-related questions', 'question': 'When will the course be offered next?', 'course': 'llm-zoomcamp'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902eb7d0-0390-4e66-b40c-86aa1522c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{'role':'user', 'content':'When does the course start?'}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4c5e88-e50a-4737-beef-9eedc7094045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Could you please specify which course you are referring to?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be51c07-5de4-4769-8804-1fd37a957bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" \n",
    "You are a course instructor. Answer the QUESTION based on the CONTEXT\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "If the answer is not in the CONTEXT then output NONE\n",
    "\n",
    "QUESTION:{question}\n",
    "\n",
    "CONTEXT:{context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "642df65e-88b1-4fe0-8273-1f5dd334f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "for doc in documents:\n",
    "    context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer:{doc['text']}]\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f39263-350d-4651-8db8-a5f6979fa899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section: General course-related questions\n",
      "question: I just discovered the course. Can I still join?\n",
      "answer:Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?\n",
      "answer:You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?\n",
      "answer:The zoom link is only published to instructors/presenters/TAs.\n",
      "Students participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\n",
      "Don’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - How do I get access?\n",
      "answer:Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\n",
      "Answer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\n",
      "Issue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\n",
      "Answer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - How many free hours do we get?\n",
      "answer:We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month\n",
      "answer:This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Cloud alternatives with GPU\n",
      "answer:Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\n",
      "Google Colab\n",
      "Kaggle\n",
      "Databricks (?), so many others.\n",
      "Use GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer:When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\n",
      "If you want to see what your Display name is, click on the Edit Course Profile button.\n",
      "First field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\n",
      "Unless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer:No, you can only get a certificate if you finish the course with a “live” cohort.\n",
      "We don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\n",
      "You can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: I missed the first homework - can I still get a certificate?\n",
      "answer:Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: I was working on next week’s homework/content - why does it keep changing?\n",
      "answer:This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: When will the course be offered next?\n",
      "answer:Summer 2025 (via Alexey).]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Are there any lectures/videos? Where are they?\n",
      "answer:Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.\n",
      "answer:Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\Users\\YourUsername\\.wslconfig) with the desired RAM allocation:\n",
      "[wsl2]\n",
      "memory=8GB\n",
      "Restart WSL: wsl --shutdown\n",
      "Run the free command to verify the changes. For more details, read this article.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error when running OpenAI chat.completions.create command\n",
      "answer:You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error: RateLimitError: Error code: 429 -\n",
      "answer:RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\n",
      "The above errors are related to your OpenAI API account’s quota.\n",
      "There is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\n",
      "Steps to resolve:\n",
      "Add credits to your account here (min $5)\n",
      "In chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\n",
      "You might need to recreate an API key after adding credits to your account and update it locally.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\n",
      "answer:Update openai version from 0.27.0 -> any 1.x version]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: How much will I have to spend to use the Open AI API?\n",
      "answer:Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Do I have to subscribe and pay for Open AI API for this course?\n",
      "answer:No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\n",
      "llm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: ElasticSearch: ERROR: Elasticsearch exited unexpectedly\n",
      "answer:If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\n",
      "I specified the RAM size to the configuration (-m 4GB)\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-m 4GB \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "Or give it _less_ RAM:\n",
      "Tip for Github Codespace users\n",
      "If you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\n",
      "In that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\n",
      "This reduces the resource usage.\n",
      "Full command:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "If it doesn't work, try this:\n",
      "sudo sysctl -w vm.max_map_count=262144\n",
      "And give the Java machine inside the container more RAM:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "--ulimit nofile=65536:65536 \\\n",
      "--ulimit memlock=-1:-1 \\\n",
      "--memory=4g \\\n",
      "--cpus=2 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "Another possible solution may be to set the memory_lock to false:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
      "-e \"bootstrap.memory_lock=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\n",
      "answer:Instead of document as used in the course video, use doc]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Docker: How do I store data persistently in Elasticsearch?\n",
      "answer:When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\n",
      "docker volume create elasticsearch_data\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-v elasticsearch_data:/usr/share/elasticsearch/data \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Authentication: Safe and easy way to store and load API keys\n",
      "answer:You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\n",
      "For example, you can create a new file named “api_keys.yml” in your repository.\n",
      "Then, do not forget to add it in your .gitignore file:\n",
      "#api_keys\n",
      "api_keys.yml\n",
      "You can now fill your api_keys.yml file:\n",
      "OPENAI_API_KEY: “sk[...]”\n",
      "GROQ_API_KEY: “gqk_[...]”\n",
      "Save your file.\n",
      "You will need the pyyaml library to load your yaml file, so run this command in your terminal:\n",
      "pip install pyyaml\n",
      "Now, open your jupyter notebook.\n",
      "You can load your yaml file and the associated keys with this code:\n",
      "import yaml\n",
      "# Open the file\n",
      "with open('api_keys.yml', 'r') as file:\n",
      "# Load the data from the file\n",
      "data = yaml.safe_load(file)\n",
      "# Get the API key (Groq example here)\n",
      "groq_api_key = data['GROQ_API_KEY']\n",
      "Now, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?\n",
      "answer:Option1: using direnv\n",
      "created the .envrc file & added my API key, ran direnv allow in the terminal\n",
      "was getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "resolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv('.envrc')\n",
      "Option 2: using Codespaces Secrets\n",
      "Log in to your GitHub account and navigate to Settings > Codespaces\n",
      "There is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\n",
      "Once you set this up, the key will be available in your codespaces session]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: How can I use Ollama open-source models locally on my pc without using any API?\n",
      "answer:Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\n",
      "To download ollama for Ubuntu:\n",
      "``` curl -fsSL https://ollama.com/install.sh | sh ```\n",
      "To download ollama for Mac and Windows, follow the guide on this link:\n",
      "https://ollama.com/download/\n",
      "Ollama a number of open-source LLMs like:\n",
      "Llama3\n",
      "Phi3\n",
      "Mistral and Mixtral\n",
      "Gemma\n",
      "Qwen\n",
      "You can explore more models on https://ollama.com/library/\n",
      "To download a model in Ollama, simply open command prompt and type:\n",
      "``` ollama run model_name ```\n",
      "e.g.\n",
      "``` ollama run phi3 ```\n",
      "It will automatically download the model and you can use it same way as above for later time.\n",
      "To use Ollama models for inference and llm-zoomcamp tasks, use the following function:\n",
      "import ollama\n",
      "def llm(prompt):\n",
      "response = ollama.chat(\n",
      "model=\"llama3\",\n",
      "messages=[{\"role\": \"user\", \"content\": prompt}]\n",
      ")\n",
      "return response['message']['content']\n",
      "For example, we can use it in the following way:\n",
      "prompt = \"When does the llm-zoomcamp course start?\"\n",
      "answer = llm(prompt)\n",
      "print(answer)]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\n",
      "answer:The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: Can I use Groq instead of OpenAI?\n",
      "answer:You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: Can I use open-source alternatives to OpenAI API?\n",
      "answer:Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Returning Empty list after filtering my query (HW Q3)\n",
      "answer:This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Saturn Cloud issues\n",
      "answer:Please see the General section or use CTRL+F to search this doc.]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?\n",
      "answer:Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\n",
      "Once you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\n",
      "1- Navigate to Your Project Directory:\n",
      "cd /home/jovyan/my_project\n",
      "2- Configure GitHub Remote to Use SSH:\n",
      "git remote set-url origin git@github.com:username/repository.git\n",
      "3- Stage, Commit and push your changes:\n",
      "git add .\n",
      "git commit -m \"Your commit message\"\n",
      "git push]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?\n",
      "answer:Clean out your cache using the following code:\n",
      "from transformers import TRANSFORMERS_CACHE\n",
      "print(TRANSFORMERS_CACHE)\n",
      "import shutil\n",
      "shutil.rmtree(TRANSFORMERS_CACHE)\n",
      "Note: Make sure to shutdown the notebook and restart the kernel]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?\n",
      "answer:Yes, you can. Here the step to follow:\n",
      "- Open a bash session in the elasticsearch container\n",
      "```bash\n",
      "docker exec -it elasticsearch bash\n",
      "```\n",
      "- Add path.repo configuration:\n",
      "```bash\n",
      "echo path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n",
      "```\n",
      "- Restart container and verify it was created correctly:\n",
      "```bash\n",
      "docker restart elasticsearch\n",
      "curl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\n",
      "```\n",
      "- Create the snapshot (this is the backup ;) )\n",
      "```bash\n",
      "curl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"your_index_name\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false\n",
      "}\n",
      "'\n",
      "```\n",
      "- Copy the backup to my machine:\n",
      "```bash\n",
      "docker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\n",
      "```\n",
      "- Now create the new container or use docker-compose just in case you are following the module 2:\n",
      "```bash\n",
      "docker compose up -d\n",
      "```\n",
      "- Add de path.repo configuration in the new one, same as before:\n",
      "```bash\n",
      "docker exec -it new_elasticsearch bash\n",
      "echo path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n",
      "```\n",
      "- Restart the docker container and copy the snapshot in it:\n",
      "```bash\n",
      "docker restart new_elasticsearch\n",
      "docker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\n",
      "```\n",
      "- Register the Snapshot Repository in the New Container:\n",
      "```bash\n",
      "curl -X PUT \"localhost:9200/_snapshot/my_backup\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"type\": \"fs\",\n",
      "\"settings\": {\n",
      "\"location\": \"/usr/share/elasticsearch/backup\"\n",
      "}\n",
      "}\n",
      "'\n",
      "```\n",
      "- Verify if it exists:\n",
      "```bash\n",
      "curl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\n",
      "```\n",
      "- Restore the snapshot:\n",
      "```bash\n",
      "curl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"your_index_name\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false\n",
      "}\n",
      "'\n",
      "```\n",
      "- Show your indexes:\n",
      "```bash\n",
      "curl -X GET \"localhost:9200/_cat/indices?v\"\n",
      "```\n",
      "- Extra point: If you want to change the original index name by other when you restore the snapshot:\n",
      "```bash\n",
      "curl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"old_index\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false,\n",
      "\"rename_pattern\": \"old_index\",\n",
      "\"rename_replacement\": \"new_index\"\n",
      "}\n",
      "'\n",
      "```]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: ElasticSearch: How can I limit the memory used by the ElasticSearch container?\n",
      "answer:You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\n",
      "- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\n",
      "- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\n",
      "services:\n",
      "elasticsearch:\n",
      "image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "container_name: elasticsearch\n",
      "environment:\n",
      "- discovery.type=single-node\n",
      "- xpack.security.enabled=false\n",
      "ports:\n",
      "- \"9200:9200\"\n",
      "- \"9300:9300\"\n",
      "deploy:\n",
      "resources:\n",
      "limits:\n",
      "cpus: '1.0'  # Limits to 1 CPU\n",
      "reservations:\n",
      "cpus: '0.5'  # Reserves 0.5 CPUs]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: How to inspect the content of a file inside a Docker container ?\n",
      "answer:You have several ways to inspect the content of a file when you are inside a Docker container.\n",
      "First, make sure you ran the docker container interactively using bash:\n",
      "docker exec -it <container> bash\n",
      "Then, you are able to use bash commands. For this case, I propose two solutions:\n",
      "Use “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\n",
      "Install vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\n",
      "apt-get install vim\n",
      "vim your_file\n",
      "Then, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: Error: Docker mounted volume adds ;C to end of windows path\n",
      "answer:Use the following line instead in mounting the current volume to docker for Q4:\n",
      "`-v \"/${PWD}/ollama_files:/root/.ollama\"`]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?\n",
      "answer:In Docker Desktop, try to increase the resource.\n",
      "Go to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\n",
      "Added by Dandy Arif Rahman]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: How can to clean docker cache?\n",
      "answer:docker system prune -a]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama\n",
      "answer:A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\n",
      "Added by Eduardo Muñoz]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: Error: NotFoundError: Error code: 404 - {'error': {'message': \"model XXX not found, try pulling it first\" …\n",
      "answer:To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\n",
      "Added by Taras Goriachko\n",
      "Ollama: Running Ollama locally on Colab gives error after the llm() line\n",
      "APIConnectionError: Connection error.\n",
      "It seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\n",
      "Found a solution in the Medium article and this link:\n",
      "https://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\n",
      "https://github.com/ollama/ollama/issues/703\n",
      "Added by Hanaa]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: How can remove Ollama model?\n",
      "answer:ollama list\n",
      "ollama rm [model_name]]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: Error code 500 InternalServerError\n",
      "answer:InternalServerError: Error code: 500 - {'error': {'message': 'model requires more system memory (5.6 GiB) than is available (1.5 GiB)', 'type': 'api_error', 'param': None, 'code': None}}.\n",
      "Running elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\n",
      "version: '3.8'\n",
      "services:\n",
      "elasticsearch:\n",
      "image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "container_name: elasticsearch\n",
      "environment:\n",
      "- discovery.type=single-node\n",
      "- xpack.security.enabled=false\n",
      "- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\n",
      "ports:\n",
      "- \"9200:9200\"\n",
      "- \"9300:9300\"\n",
      "deploy:\n",
      "resources:\n",
      "limits:\n",
      "memory: 2G  # change 2\n",
      "ollama:\n",
      "image: ollama/ollama\n",
      "container_name: ollama\n",
      "volumes:\n",
      "- ollama:/root/.ollama\n",
      "ports:\n",
      "- \"11434:11434\"\n",
      "volumes:\n",
      "ollama:\n",
      "Added by Zoe Zelkha]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF\n",
      "answer:Manually set the token as below:\n",
      "access_token = <your_token>\n",
      "model  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\n",
      "answer:To solve just install transformers directly from github\n",
      "!pip install git+https://github.com/huggingface/transformers]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3\n",
      "answer:To solve just install transformers directly from github\n",
      "!pip install git+https://github.com/huggingface/transformers]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: from google.protobuf.pyext import _message / TypeError: bases must be types\n",
      "answer:pip install protobuf==3.20.1\n",
      "Added by Ibai Irastorza]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?\n",
      "answer:1. search with the model name on hugging face.\n",
      "2. get the transformer used on the model.\n",
      "3. using the transformer, encode the string you want.\n",
      "4. calculate the length of the outputted tensor.\n",
      "The previous code snippet uses the tokenizer of google/gemma-2b LLM. \n",
      "Don’t forget to make your token secret.\n",
      "Added by kamal]\n",
      "\n",
      "section: Module 3: X\n",
      "question: How to run a model using CUDA for GPU usage?\n",
      "answer:The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\n",
      "In my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\n",
      "To install all the needed packages use this command:\n",
      "!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\n",
      "And after that just executed this command:\n",
      "!pip install --upgrade transformers]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\n",
      "answer:Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\n",
      "Or use the keyword ‘body’ instead of ‘document’\n",
      "For conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\n",
      "\n",
      "$ conda config --add channels conda-forge\n",
      "$ conda config --set channel_priority strict\n",
      "$ conda install -c conda-forge elasticsearch==8.4.3]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\n",
      "answer:This worked for me:]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search\n",
      "answer:Try to running docker container based on first course module like this :\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "And don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?\n",
      "answer:As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\n",
      "The KNN related score, which is between -1 and 1 (cosine similarity)\n",
      "The text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document's relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\n",
      "Since we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\n",
      "To get more details about the final scores, you can modify the search query and add an “explain” parameter:\n",
      "response = es_client.search(\n",
      "index=index_name,\n",
      "query={\n",
      "\"match\": {\"section\": \"General course-related questions\"},\n",
      "},\n",
      "knn=knn_query,\n",
      "size=5,\n",
      "explain=True\n",
      ")\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Not module named “sentence_transformers”\n",
      "answer:For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\n",
      "pip install sentence-transformers]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Can not create the index: Connection timeout.\n",
      "answer:I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\n",
      "I checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\n",
      "My problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\n",
      "docker image prune\n",
      "docker volume prune\n",
      "docker container prune\n",
      "Added by Ibai Irastorza]\n",
      "\n",
      "section: Module 3: X\n",
      "question: TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\n",
      "answer:Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\n",
      "v_query = embedding_model.encode(query_text)\n",
      "results = search_function(v_query)]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Find maximum of an numpy array (of any dimension):\n",
      "answer:max_value = numpy_array.max()]\n",
      "\n",
      "section: Module 3: X\n",
      "question: What is the cosine similarity?\n",
      "answer:Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.]\n",
      "\n",
      "section: Module 3: X\n",
      "question: What are documents in ElasticSearch?\n",
      "answer:A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See 'docker run --help'.\n",
      "answer:docker stop elasticsearch\n",
      "docker rm elasticsearch\n",
      "How to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\n",
      "score = (es_score - 0.5) * 2]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: Warning: 'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet' how to suppress?\n",
      "answer:Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.\n",
      "answer:Solution 1 : Install Visual C++ Redistributable\n",
      "Solution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?\n",
      "answer:Inside .env file change POSTGRES_HOST=localhost]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook\n",
      "answer:By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\n",
      "The instruction to show the results must be preceded by:\n",
      "pd.set_option('display.max_colwidth', None)\n",
      "Here are the specs for the display_max_colwidth option, as describide in the official docs:\n",
      "display.max_colwidth : int or None\n",
      "The maximum width in characters of a column in the repr of\n",
      "a pandas data structure. When the column overflows, a \"...\"\n",
      "placeholder is embedded in the output. A 'None' value means unlimited.\n",
      "[default: 50] [currently: 50]]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?\n",
      "answer:import numpy as np\n",
      "normalize_vec = lambda v: v / np.linalg.norm(v)\n",
      "df[\"new_col\"] = df[\"org_col\"].apply(norm_vec)]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?\n",
      "answer:To compute the 75% percentile or 0.75 quantile:\n",
      "quantile: int = df[\"col\"].quantile(q=0.75)]\n",
      "\n",
      "section: Module 5: X\n",
      "question: How can I remove all Docker containers, images, and volumes, and builds from the terminal?\n",
      "answer:1. Delete all containers (including running ones):\n",
      "```\n",
      "docker rm -f\n",
      "```\n",
      "2. Remove all images:\n",
      "```\n",
      "docker rmi -f\n",
      "```\n",
      "3. Delete all volumes:\n",
      "```\n",
      "docker volume rm\n",
      "```]\n",
      "\n",
      "section: Module 5: X\n",
      "question: I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\n",
      "answer:Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>]\n",
      "\n",
      "section: Module 6: X\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Module 6: X\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Is it a group project?\n",
      "answer:No, the capstone is a solo project.]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Do we submit 2 projects, what does attempt 1 and 2 mean?\n",
      "answer:You only need to submit 1 project. \n",
      "If the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\n",
      "If you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\n",
      "If you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\n",
      "Remember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Does the competition count as the capstone?\n",
      "answer:No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.]\n",
      "\n",
      "section: Capstone Project\n",
      "question: How is my capstone project going to be evaluated?\n",
      "answer:Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\n",
      "You will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\n",
      "The final grade you get will be the median score of the grades you get from the peer reviewers.\n",
      "And of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).]\n",
      "\n",
      "section: Certificates\n",
      "question: Do I have to use ElasticSearch or X library?\n",
      "answer:Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Can I use the workshop materials for my own projects or share them with others?\n",
      "answer:Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "answer:The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "answer:Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: How do I know which tables are in the db\n",
      "answer:You can use the db.table_names() to list all the tables in the db]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Does DLT have connectors to ClickHouse or StarRocks?\n",
      "answer:Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search\n",
      "answer:If you get this error\n",
      "Or 401 Client Error , then you either need to grant access to the key or the key is wrong.]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?\n",
      "answer:Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Connection refused error on prompting the ollam RAG?\n",
      "answer:If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\n",
      "If you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\n",
      "Added by Abiodun Gbadamosi]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Question\n",
      "answer:Answer]\n"
     ]
    }
   ],
   "source": [
    "print(context.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f085a0-b1cb-4582-9fe1-f0cd190c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question='Will I get a certificate??',\n",
    "                       context=context).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea0af818-5309-4851-ab1c-85f05d1e47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a course instructor. Answer the QUESTION based on the CONTEXT\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "If the answer is not in the CONTEXT then output NONE\n",
      "\n",
      "QUESTION:Will I get a certificate??\n",
      "\n",
      "CONTEXT:section: General course-related questions\n",
      "question: I just discovered the course. Can I still join?\n",
      "answer:Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the [insert-zoomcamp-name]. When can I expect to receive the confirmation email?\n",
      "answer:You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is Open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?\n",
      "answer:The zoom link is only published to instructors/presenters/TAs.\n",
      "Students participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\n",
      "Don’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - How do I get access?\n",
      "answer:Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\n",
      "Answer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\n",
      "Issue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\n",
      "Answer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - How many free hours do we get?\n",
      "answer:We get 15 free hours per month, which might be limited to the free tier’s hardware configuration.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: SaturnCloud - Something went wrong. Max of 15 hours of resource usage per month\n",
      "answer:This message means you have used all allocated hours. Make sure to set Shutout After in settings. Also, do not leave your notebooks running. If your hours are out, try using Google Colab and Kaggle.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Cloud alternatives with GPU\n",
      "answer:Check the quota and reset cycle carefully - is the free hours per month or per week? Usually if you change the configuration, the free hours quota might also be adjusted,or it might be billed separately.\n",
      "Google Colab\n",
      "Kaggle\n",
      "Databricks (?), so many others.\n",
      "Use GPTs to find out. Some might have restrictions on what you can and cannot install, so be sure to read what is included in a free vs paid tier.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer:When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\n",
      "If you want to see what your Display name is, click on the Edit Course Profile button.\n",
      "First field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\n",
      "Unless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer:No, you can only get a certificate if you finish the course with a “live” cohort.\n",
      "We don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\n",
      "You can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: I missed the first homework - can I still get a certificate?\n",
      "answer:Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: I was working on next week’s homework/content - why does it keep changing?\n",
      "answer:This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.]\n",
      "\n",
      "section: General course-related questions\n",
      "question: When will the course be offered next?\n",
      "answer:Summer 2025 (via Alexey).]\n",
      "\n",
      "section: General course-related questions\n",
      "question: Are there any lectures/videos? Where are they?\n",
      "answer:Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: WSL2 - ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.\n",
      "answer:Your WSL2 is set to use Y.Y GiB, not all your computer memory. Create .wslconfig file under your Windows user profile directory (C:\\Users\\YourUsername\\.wslconfig) with the desired RAM allocation:\n",
      "[wsl2]\n",
      "memory=8GB\n",
      "Restart WSL: wsl --shutdown\n",
      "Run the free command to verify the changes. For more details, read this article.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error when running OpenAI chat.completions.create command\n",
      "answer:You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error: RateLimitError: Error code: 429 -\n",
      "answer:RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\n",
      "The above errors are related to your OpenAI API account’s quota.\n",
      "There is no free usage of OpenAI’s API so you will be required to add funds using a credit card (see pay as you go in the OpenAI settings at platform.openai.com). Once added, re-run your python command and you should receive a successful return code.\n",
      "Steps to resolve:\n",
      "Add credits to your account here (min $5)\n",
      "In chat.completions.create(model='gpt-4o', …) specify one of the available for you models:\n",
      "You might need to recreate an API key after adding credits to your account and update it locally.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?\n",
      "answer:Update openai version from 0.27.0 -> any 1.x version]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: How much will I have to spend to use the Open AI API?\n",
      "answer:Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenAI: Do I have to subscribe and pay for Open AI API for this course?\n",
      "answer:No, you don't have to pay for this service in order to complete the course homeworks, you could use some of the alternatives free from this list posted into the course Github.\n",
      "llm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: ElasticSearch: ERROR: Elasticsearch exited unexpectedly\n",
      "answer:If you get this error, it’s likely that elasticsearch doesn’t get enough RAM\n",
      "I specified the RAM size to the configuration (-m 4GB)\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-m 4GB \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "Or give it _less_ RAM:\n",
      "Tip for Github Codespace users\n",
      "If you want to run elasticsearch server in a docker, then it may fail with the command in the documentation.\n",
      "In that case, you can try inserting this line -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\".\n",
      "This reduces the resource usage.\n",
      "Full command:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "If it doesn't work, try this:\n",
      "sudo sysctl -w vm.max_map_count=262144\n",
      "And give the Java machine inside the container more RAM:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "--ulimit nofile=65536:65536 \\\n",
      "--ulimit memlock=-1:-1 \\\n",
      "--memory=4g \\\n",
      "--cpus=2 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms2g -Xmx2g\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "Another possible solution may be to set the memory_lock to false:\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "-e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
      "-e \"bootstrap.memory_lock=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'\n",
      "answer:Instead of document as used in the course video, use doc]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Docker: How do I store data persistently in Elasticsearch?\n",
      "answer:When you stop the container, the data you previously added to elastic will be gone. To avoid it, we can add volume mapping:\n",
      "docker volume create elasticsearch_data\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-v elasticsearch_data:/usr/share/elasticsearch/data \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Authentication: Safe and easy way to store and load API keys\n",
      "answer:You can store your different API keys in a yaml file that you will add in your .gitignore file. Be careful to never push or share this file.\n",
      "For example, you can create a new file named “api_keys.yml” in your repository.\n",
      "Then, do not forget to add it in your .gitignore file:\n",
      "#api_keys\n",
      "api_keys.yml\n",
      "You can now fill your api_keys.yml file:\n",
      "OPENAI_API_KEY: “sk[...]”\n",
      "GROQ_API_KEY: “gqk_[...]”\n",
      "Save your file.\n",
      "You will need the pyyaml library to load your yaml file, so run this command in your terminal:\n",
      "pip install pyyaml\n",
      "Now, open your jupyter notebook.\n",
      "You can load your yaml file and the associated keys with this code:\n",
      "import yaml\n",
      "# Open the file\n",
      "with open('api_keys.yml', 'r') as file:\n",
      "# Load the data from the file\n",
      "data = yaml.safe_load(file)\n",
      "# Get the API key (Groq example here)\n",
      "groq_api_key = data['GROQ_API_KEY']\n",
      "Now, you can easily replace the “api_key” value directly with the loaded values without loading your environment variables.\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Authentication: Why is my OPENAI_API_KEY not found in the jupyter notebook?\n",
      "answer:Option1: using direnv\n",
      "created the .envrc file & added my API key, ran direnv allow in the terminal\n",
      "was getting an error: \"OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n",
      "resolution: install dotenv & add the following to a cell in the notebook. You can install dotenv by running: pip install python-dotenv.\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv('.envrc')\n",
      "Option 2: using Codespaces Secrets\n",
      "Log in to your GitHub account and navigate to Settings > Codespaces\n",
      "There is a section called secrets where you can create Secrets like OPENAI_API_KEY and select for which repositories the secret is supposed to be available.\n",
      "Once you set this up, the key will be available in your codespaces session]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: How can I use Ollama open-source models locally on my pc without using any API?\n",
      "answer:Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\n",
      "To download ollama for Ubuntu:\n",
      "``` curl -fsSL https://ollama.com/install.sh | sh ```\n",
      "To download ollama for Mac and Windows, follow the guide on this link:\n",
      "https://ollama.com/download/\n",
      "Ollama a number of open-source LLMs like:\n",
      "Llama3\n",
      "Phi3\n",
      "Mistral and Mixtral\n",
      "Gemma\n",
      "Qwen\n",
      "You can explore more models on https://ollama.com/library/\n",
      "To download a model in Ollama, simply open command prompt and type:\n",
      "``` ollama run model_name ```\n",
      "e.g.\n",
      "``` ollama run phi3 ```\n",
      "It will automatically download the model and you can use it same way as above for later time.\n",
      "To use Ollama models for inference and llm-zoomcamp tasks, use the following function:\n",
      "import ollama\n",
      "def llm(prompt):\n",
      "response = ollama.chat(\n",
      "model=\"llama3\",\n",
      "messages=[{\"role\": \"user\", \"content\": prompt}]\n",
      ")\n",
      "return response['message']['content']\n",
      "For example, we can use it in the following way:\n",
      "prompt = \"When does the llm-zoomcamp course start?\"\n",
      "answer = llm(prompt)\n",
      "print(answer)]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?\n",
      "answer:The question asks for the number of tokens in gpt-4o model. tiktoken is a python library that can be used to get the number of tokens. You don't need openai api key to to get the number of tokens. You can use the code provided in the question to get the number of tokens.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: Can I use Groq instead of OpenAI?\n",
      "answer:You can use any LLM platform for your experiments and your project. Also, the homework is designed in such a way that you don’t need to have access to any paid services and can do it locally. However, you would need to adjust the code for that platform. See their documentation pages.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: OpenSource: Can I use open-source alternatives to OpenAI API?\n",
      "answer:Yes. See module 2 and the open-ai-alternatives.md in module 1 folder.]\n",
      "\n",
      "section: Module 1: Introduction\n",
      "question: Returning Empty list after filtering my query (HW Q3)\n",
      "answer:This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Saturn Cloud issues\n",
      "answer:Please see the General section or use CTRL+F to search this doc.]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: SaturnCloud: How do you manage the changes from SaturnCloud to your Github repository?\n",
      "answer:Of course you should have first added your Github repository in SaturnCloud and the SSH Key in your Github account settings.\n",
      "Once you are in jupyter notebook from SaturnCloud, open the terminal and write these lines:\n",
      "1- Navigate to Your Project Directory:\n",
      "cd /home/jovyan/my_project\n",
      "2- Configure GitHub Remote to Use SSH:\n",
      "git remote set-url origin git@github.com:username/repository.git\n",
      "3- Stage, Commit and push your changes:\n",
      "git add .\n",
      "git commit -m \"Your commit message\"\n",
      "git push]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?\n",
      "answer:Clean out your cache using the following code:\n",
      "from transformers import TRANSFORMERS_CACHE\n",
      "print(TRANSFORMERS_CACHE)\n",
      "import shutil\n",
      "shutil.rmtree(TRANSFORMERS_CACHE)\n",
      "Note: Make sure to shutdown the notebook and restart the kernel]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: ElasticSearch: Can I backup and restore my elasticsearch index from one to another docker container?\n",
      "answer:Yes, you can. Here the step to follow:\n",
      "- Open a bash session in the elasticsearch container\n",
      "```bash\n",
      "docker exec -it elasticsearch bash\n",
      "```\n",
      "- Add path.repo configuration:\n",
      "```bash\n",
      "echo path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n",
      "```\n",
      "- Restart container and verify it was created correctly:\n",
      "```bash\n",
      "docker restart elasticsearch\n",
      "curl -X GET \"localhost:9200/_snapshot/my_backup?pretty\"\n",
      "```\n",
      "- Create the snapshot (this is the backup ;) )\n",
      "```bash\n",
      "curl -X PUT \"localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"your_index_name\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false\n",
      "}\n",
      "'\n",
      "```\n",
      "- Copy the backup to my machine:\n",
      "```bash\n",
      "docker cp elasticsearch:/usr/share/elasticsearch/backup /path/to/local\n",
      "```\n",
      "- Now create the new container or use docker-compose just in case you are following the module 2:\n",
      "```bash\n",
      "docker compose up -d\n",
      "```\n",
      "- Add de path.repo configuration in the new one, same as before:\n",
      "```bash\n",
      "docker exec -it new_elasticsearch bash\n",
      "echo path.repo: [\"/usr/share/elasticsearch/backup\"] >> /usr/share/elasticsearch/config/elasticsearch.yml\n",
      "```\n",
      "- Restart the docker container and copy the snapshot in it:\n",
      "```bash\n",
      "docker restart new_elasticsearch\n",
      "docker cp /path/to/local/backup new_elasticsearch:/usr/share/elasticsearch\n",
      "```\n",
      "- Register the Snapshot Repository in the New Container:\n",
      "```bash\n",
      "curl -X PUT \"localhost:9200/_snapshot/my_backup\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"type\": \"fs\",\n",
      "\"settings\": {\n",
      "\"location\": \"/usr/share/elasticsearch/backup\"\n",
      "}\n",
      "}\n",
      "'\n",
      "```\n",
      "- Verify if it exists:\n",
      "```bash\n",
      "curl -X GET \"localhost:9200/_snapshot/my_backup/snapshot_1?pretty\"\n",
      "```\n",
      "- Restore the snapshot:\n",
      "```bash\n",
      "curl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"your_index_name\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false\n",
      "}\n",
      "'\n",
      "```\n",
      "- Show your indexes:\n",
      "```bash\n",
      "curl -X GET \"localhost:9200/_cat/indices?v\"\n",
      "```\n",
      "- Extra point: If you want to change the original index name by other when you restore the snapshot:\n",
      "```bash\n",
      "curl -X POST \"localhost:9200/_snapshot/my_backup/snapshot_1/_restore?pretty\" -H 'Content-Type: application/json' -d'\n",
      "{\n",
      "\"indices\": \"old_index\",\n",
      "\"ignore_unavailable\": true,\n",
      "\"include_global_state\": false,\n",
      "\"rename_pattern\": \"old_index\",\n",
      "\"rename_replacement\": \"new_index\"\n",
      "}\n",
      "'\n",
      "```]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: ElasticSearch: How can I limit the memory used by the ElasticSearch container?\n",
      "answer:You can limit the amount of memory used in the ElasticSearch container by adding the next line to the environment section of your docker-compose. Choose the amount of your preference, e.g.:\n",
      "- \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"  # Set Java heap size to 1GB\n",
      "- You can limit CPU usage for an Elasticsearch service within a docker-compose.yaml file, you can utilize the resource configuration options available in Docker Compose. This includes cpus to limit the number of CPUs that the container can utilize. You can configure your Elasticsearch section in the docker-compose.yaml to restrict CPU usage:\n",
      "services:\n",
      "elasticsearch:\n",
      "image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "container_name: elasticsearch\n",
      "environment:\n",
      "- discovery.type=single-node\n",
      "- xpack.security.enabled=false\n",
      "ports:\n",
      "- \"9200:9200\"\n",
      "- \"9300:9300\"\n",
      "deploy:\n",
      "resources:\n",
      "limits:\n",
      "cpus: '1.0'  # Limits to 1 CPU\n",
      "reservations:\n",
      "cpus: '0.5'  # Reserves 0.5 CPUs]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: How to inspect the content of a file inside a Docker container ?\n",
      "answer:You have several ways to inspect the content of a file when you are inside a Docker container.\n",
      "First, make sure you ran the docker container interactively using bash:\n",
      "docker exec -it <container> bash\n",
      "Then, you are able to use bash commands. For this case, I propose two solutions:\n",
      "Use “cat” and the file you want to see the content: cat your_file . This will directly print the content in your terminal.\n",
      "Install vim or nano using apt get and open the file using vim or nano (this can be more suitable for larger files):\n",
      "apt-get install vim\n",
      "vim your_file\n",
      "Then, you can exit your file in vim by pressing ESC then typing “:q” and finally press ENTER\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: Error: Docker mounted volume adds ;C to end of windows path\n",
      "answer:Use the following line instead in mounting the current volume to docker for Q4:\n",
      "`-v \"/${PWD}/ollama_files:/root/.ollama\"`]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: Why does inferring using Phi 3 locally take so long on Macbook Air M1?\n",
      "answer:In Docker Desktop, try to increase the resource.\n",
      "Go to the Dashboard > Settings > Resources. Raise the memory limit to 15GB and swap to 4GB - be generous. Applied and restarted the changes\n",
      "Added by Dandy Arif Rahman]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Docker: How can to clean docker cache?\n",
      "answer:docker system prune -a]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: “Error: pull model manifest: 503: no healthy upstream” when pulling a model with Ollama\n",
      "answer:A network connection failure usually causes this error and if you try to repeat the operation immediately it’ll still fail. It’s a temporary error, you should wait for 2 or 3 minutes before attempting to pull the model again. Then some minutes later, the operation will success.\n",
      "Added by Eduardo Muñoz]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: Error: NotFoundError: Error code: 404 - {'error': {'message': \"model XXX not found, try pulling it first\" …\n",
      "answer:To solve this you need to pull one of these models first: https://ollama.com/library . Also check the proper name of the module.\n",
      "Added by Taras Goriachko\n",
      "Ollama: Running Ollama locally on Colab gives error after the llm() line\n",
      "APIConnectionError: Connection error.\n",
      "It seems to be running at localhost:11434 however localhost:11434/v1/ gives 404\n",
      "Found a solution in the Medium article and this link:\n",
      "https://medium.com/@mauryaanoop3/running-ollama-on-google-colab-free-tier-a-step-by-step-guide-9ef74b1f8f7a\n",
      "https://github.com/ollama/ollama/issues/703\n",
      "Added by Hanaa]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: How can remove Ollama model?\n",
      "answer:ollama list\n",
      "ollama rm [model_name]]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Ollama: Error code 500 InternalServerError\n",
      "answer:InternalServerError: Error code: 500 - {'error': {'message': 'model requires more system memory (5.6 GiB) than is available (1.5 GiB)', 'type': 'api_error', 'param': None, 'code': None}}.\n",
      "Running elastic search with the docker-compose is the cause of the RAM memory issue. To fix this you need to change the docker-compose.yaml file to limit the RAM usage of elastic search\n",
      "version: '3.8'\n",
      "services:\n",
      "elasticsearch:\n",
      "image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "container_name: elasticsearch\n",
      "environment:\n",
      "- discovery.type=single-node\n",
      "- xpack.security.enabled=false\n",
      "- ES_JAVA_OPTS=-Xms1g -Xmx1g  # change 1\n",
      "ports:\n",
      "- \"9200:9200\"\n",
      "- \"9300:9300\"\n",
      "deploy:\n",
      "resources:\n",
      "limits:\n",
      "memory: 2G  # change 2\n",
      "ollama:\n",
      "image: ollama/ollama\n",
      "container_name: ollama\n",
      "volumes:\n",
      "- ollama:/root/.ollama\n",
      "ports:\n",
      "- \"11434:11434\"\n",
      "volumes:\n",
      "ollama:\n",
      "Added by Zoe Zelkha]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF\n",
      "answer:Manually set the token as below:\n",
      "access_token = <your_token>\n",
      "model  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: Error: ModuleNotFoundError: No module named 'transformers.cache_utils'\n",
      "answer:To solve just install transformers directly from github\n",
      "!pip install git+https://github.com/huggingface/transformers]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3\n",
      "answer:To solve just install transformers directly from github\n",
      "!pip install git+https://github.com/huggingface/transformers]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: Python: from google.protobuf.pyext import _message / TypeError: bases must be types\n",
      "answer:pip install protobuf==3.20.1\n",
      "Added by Ibai Irastorza]\n",
      "\n",
      "section: Module 2: Open-Source LLMs\n",
      "question: HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?\n",
      "answer:1. search with the model name on hugging face.\n",
      "2. get the transformer used on the model.\n",
      "3. using the transformer, encode the string you want.\n",
      "4. calculate the length of the outputted tensor.\n",
      "The previous code snippet uses the tokenizer of google/gemma-2b LLM. \n",
      "Don’t forget to make your token secret.\n",
      "Added by kamal]\n",
      "\n",
      "section: Module 3: X\n",
      "question: How to run a model using CUDA for GPU usage?\n",
      "answer:The last version I checked for CUDA was 12.5 using a cloud environment like Saturn Cloud. Then the torch package for python should be on supported for that version of CUDA, is followed by cu121 which means that version of torch supports cuda 12.1. Check this page to find the package and version available for CUDA (remember to search the keyword “cu”\n",
      "In my case I focused on using a torch==2.3.1 and the last cuda version supported was 12.1 (it works on Saturn Cloud)\n",
      "To install all the needed packages use this command:\n",
      "!pip install transformers accelerate torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --trusted-host download.pytorch.org --index-url https://download.pytorch.org/whl/cu121\n",
      "And after that just executed this command:\n",
      "!pip install --upgrade transformers]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: Error: Elasticsearch.index() got an unexpected keyword argument 'document'\n",
      "answer:Upgrade elasticsearch 7.13.3 to 8.14.0 or any 7.x installation to 8.x. The earlier modules used a docker image of elasticsearch 8.4.3 so the python installation of elasticsearch must also be at least 8.x.\n",
      "Or use the keyword ‘body’ instead of ‘document’\n",
      "For conda users, if you’re trying to update to elasticsearch 8.x using conda install elasticsearch==8.4.3  but getting a “PackagesNotFoundError\", try this:\n",
      "\n",
      "$ conda config --add channels conda-forge\n",
      "$ conda config --set channel_priority strict\n",
      "$ conda install -c conda-forge elasticsearch==8.4.3]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: TypeError: Elasticsearch.search() got an unexpected keyword argument 'knn'\n",
      "answer:This worked for me:]\n",
      "\n",
      "section: Module 3: X\n",
      "question: ElasticSearch: ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7c455bb94ac0>: Failed to establish a new connection: [Errno 111] Connection refused)) in elastic search\n",
      "answer:Try to running docker container based on first course module like this :\n",
      "docker run -it \\\n",
      "--rm \\\n",
      "--name elasticsearch \\\n",
      "-p 9200:9200 \\\n",
      "-p 9300:9300 \\\n",
      "-e \"discovery.type=single-node\" \\\n",
      "-e \"xpack.security.enabled=false\" \\\n",
      "docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
      "And don’t forget to forwarding your port 9200 if you’re using github codespace or run locally in vscode]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Why do I get scores greater than 1 on my hits after querying my ElasticSearch database ?\n",
      "answer:As seen in this video: https://www.youtube.com/watch?v=ptByfB_YcEg&t=102s, we can get scores on obtained hits that are greater than 1 despite having a “cosine” similarity measure in our index settings. We would thus expect scores between -1 and 1. However, in the case of the final query, we have several scores additionned together to provide the final score:\n",
      "The KNN related score, which is between -1 and 1 (cosine similarity)\n",
      "The text relevance score:  BM25 algorithm scores which can be any positive number, including above 1. This is a “ranking function which calculates score to represent a document's relevance with respect to query” (source: https://stackoverflow.com/questions/43794749/what-is-bm25-and-why-elasticsearch-chose-this-algorithm-for-scoring-in-version-5).\n",
      "Since we have a “match” filter in our query, this triggers the usage of the BM25 ranking algorithm and the final score contains this information.\n",
      "To get more details about the final scores, you can modify the search query and add an “explain” parameter:\n",
      "response = es_client.search(\n",
      "index=index_name,\n",
      "query={\n",
      "\"match\": {\"section\": \"General course-related questions\"},\n",
      "},\n",
      "knn=knn_query,\n",
      "size=5,\n",
      "explain=True\n",
      ")\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Not module named “sentence_transformers”\n",
      "answer:For this module homework make sure you install the package sentence-transformers it can be installed as simply as:\n",
      "pip install sentence-transformers]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Can not create the index: Connection timeout.\n",
      "answer:I was getting this error at this step: es_client.indices.create(index=index_name, body=index_settings)\n",
      "I checked the log of the elasticsearch server and running this command, the status was red: curl -X GET \"http://localhost:9200/_cluster/health?pretty\"\n",
      "My problem was that I did not have enough disk space in my computer for docker images. I ended up removing unused ones, manually and pruning:\n",
      "docker image prune\n",
      "docker volume prune\n",
      "docker container prune\n",
      "Added by Ibai Irastorza]\n",
      "\n",
      "section: Module 3: X\n",
      "question: TypeError: unsupported operand type(s) for *: 'float' and 'dict' when running the vector search function within the evaluate function\n",
      "answer:Make sure your search function receives a query vector, not a dictionary. To resolve this, ensure that the q passed to the search_function within evaluate is correctly transformed into an embedding vector. The following code can help:\n",
      "v_query = embedding_model.encode(query_text)\n",
      "results = search_function(v_query)]\n",
      "\n",
      "section: Module 3: X\n",
      "question: Find maximum of an numpy array (of any dimension):\n",
      "answer:max_value = numpy_array.max()]\n",
      "\n",
      "section: Module 3: X\n",
      "question: What is the cosine similarity?\n",
      "answer:Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.]\n",
      "\n",
      "section: Module 3: X\n",
      "question: What are documents in ElasticSearch?\n",
      "answer:A “document” is a collection of fields, which are the key-value pairs that contain your data, that have been serialized as a JSON object.]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: runing docker docker: Error response from daemon: Conflict. The container name \"/elasticsearch\" is already in use by container \"20467e6723d78ff2e4e9e0c9a8b9580c07f070e4c852d12c585b1d71aefd6665\". You have to remove (or rename) that container to be able to reuse that name. See 'docker run --help'.\n",
      "answer:docker stop elasticsearch\n",
      "docker rm elasticsearch\n",
      "How to scale Elastic search scores from [0, 1] to [-1, 1] to compare its results with your own ones, example calculating ranks using dot_product metric ?\n",
      "score = (es_score - 0.5) * 2]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: Warning: 'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet' how to suppress?\n",
      "answer:Upgrade `sentence-transformers` to v3.0.0>= e.x pip install sentence-transformers>=3.0.0 to avoid the warnings]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: In Windows OS : OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.\n",
      "answer:Solution 1 : Install Visual C++ Redistributable\n",
      "Solution 2 : Install Visual Studio, not Visual Studio Code. Like in this depicted below and restart your system. For more details, please follow this link : https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: OperationalError when running python prep.pypsycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?\n",
      "answer:Inside .env file change POSTGRES_HOST=localhost]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook\n",
      "answer:By default, in the dataframe visualization, Pandas truncate the text content in a column to 50 characters. In order to view the entire explanation given by the judge llm for a NON RELEVANT answer, as in figure:\n",
      "The instruction to show the results must be preceded by:\n",
      "pd.set_option('display.max_colwidth', None)\n",
      "Here are the specs for the display_max_colwidth option, as describide in the official docs:\n",
      "display.max_colwidth : int or None\n",
      "The maximum width in characters of a column in the repr of\n",
      "a pandas data structure. When the column overflows, a \"...\"\n",
      "placeholder is embedded in the output. A 'None' value means unlimited.\n",
      "[default: 50] [currently: 50]]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?\n",
      "answer:import numpy as np\n",
      "normalize_vec = lambda v: v / np.linalg.norm(v)\n",
      "df[\"new_col\"] = df[\"org_col\"].apply(norm_vec)]\n",
      "\n",
      "section: Module 4: Monitoring\n",
      "question: How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?\n",
      "answer:To compute the 75% percentile or 0.75 quantile:\n",
      "quantile: int = df[\"col\"].quantile(q=0.75)]\n",
      "\n",
      "section: Module 5: X\n",
      "question: How can I remove all Docker containers, images, and volumes, and builds from the terminal?\n",
      "answer:1. Delete all containers (including running ones):\n",
      "```\n",
      "docker rm -f\n",
      "```\n",
      "2. Remove all images:\n",
      "```\n",
      "docker rmi -f\n",
      "```\n",
      "3. Delete all volumes:\n",
      "```\n",
      "docker volume rm\n",
      "```]\n",
      "\n",
      "section: Module 5: X\n",
      "question: I have reached the orchestration pipeline's export and I’m facing a connection error at the stage of exporting to the vector database. Can someone help with the connection string?\n",
      "answer:Use the service name and port provided in the docker-compose.yaml file for the elasticsearch, e.g <http://><docker-compose-service-name>:<port> <http://elasticsearch:9200>]\n",
      "\n",
      "section: Module 6: X\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Module 6: X\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Is it a group project?\n",
      "answer:No, the capstone is a solo project.]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Do we submit 2 projects, what does attempt 1 and 2 mean?\n",
      "answer:You only need to submit 1 project. \n",
      "If the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\n",
      "If you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\n",
      "If you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\n",
      "Remember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort]\n",
      "\n",
      "section: Capstone Project\n",
      "question: Does the competition count as the capstone?\n",
      "answer:No, it does not (answered in office hours Jul 1st, 2024). You can participate in the math-kaggle-llm-competition as a group if you want to form teams; but capstone is an individual attempt.]\n",
      "\n",
      "section: Capstone Project\n",
      "question: How is my capstone project going to be evaluated?\n",
      "answer:Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\n",
      "You will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\n",
      "The final grade you get will be the median score of the grades you get from the peer reviewers.\n",
      "And of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).]\n",
      "\n",
      "section: Certificates\n",
      "question: Do I have to use ElasticSearch or X library?\n",
      "answer:Answer: No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project.]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Question\n",
      "answer:Answer]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Can I use the workshop materials for my own projects or share them with others?\n",
      "answer:Since dlt is open-source, we can use the content of this workshop for a capstone project. Since the main goal of dlt is to load and store data easily, we can even use it for other zoomcamps (mlops zoomcamp project for example). Do not hesitate to ask questions or use it directly in your projects.\n",
      "Added by Mélanie Fouesnard]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "answer:The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)\n",
      "answer:Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: How do I know which tables are in the db\n",
      "answer:You can use the db.table_names() to list all the tables in the db]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Does DLT have connectors to ClickHouse or StarRocks?\n",
      "answer:Currently, DLT does not have connectors for ClickHouse or StarRocks but are open to contributions from the community to add these connectors.]\n",
      "\n",
      "section: Workshops: dlthub\n",
      "question: Notebook does not have secret access or 401 Client Error: Unauthorized for url: https://api.notion.com/v1/search\n",
      "answer:If you get this error\n",
      "Or 401 Client Error , then you either need to grant access to the key or the key is wrong.]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?\n",
      "answer:Install directly from source E.g `pip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"`]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Connection refused error on prompting the ollam RAG?\n",
      "answer:If you get this error while doing the homework , simply restart the ollama server using nohup y running this line of the notebook !nohup ollama serve > nohup.out 2>&1 &\n",
      "If you do stop and restart the cell, you will need to rerun the cell containing ollama serve first.\n",
      "Added by Abiodun Gbadamosi]\n",
      "\n",
      "section: Workshops: X\n",
      "question: Question\n",
      "answer:Answer]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e216c634-2204-498a-8305-2ad74466e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the content in chatgpt\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{'role':'user', 'content':prompt}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a95da1c9-2d8f-49c0-8330-0622f73ba5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you will get a certificate, but only if you submit your project while submissions are still being accepted. Additionally, you can only receive a certificate if you finish the course with a \"live\" cohort; self-paced mode does not qualify. It is also necessary to pass the Capstone project and participate in the peer-review of three capstone projects after submitting your own project.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c4589f-bc03-4f12-a106-ddced12eba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question':3.0, 'section':0.5}\n",
    "    result = index.search(\n",
    "    query=query,\n",
    "    #If there is more than one course : filter_dict={'course':'data-engineering-zoomcamp'}, \n",
    "    boost_dict=boost,\n",
    "    num_results=20)\n",
    "    return result\n",
    "\n",
    "def prompt(result, query):\n",
    "    prompt_template = \"\"\" \n",
    "    You are a course instructor. Answer the QUESTION based on the CONTEXT\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    If the answer is not in the CONTEXT then output NONE\n",
    "    \n",
    "    QUESTION:{question}\n",
    "    \n",
    "    CONTEXT:{context}\n",
    "    \"\"\".strip()\n",
    "    context = \"\"\n",
    "    for doc in result:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer:{doc['text']}]\\n\\n\"\n",
    "    prompt = prompt_template.format(question='Will I get a certificate??',\n",
    "                       context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def run_llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{'role':'user', 'content':prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def rag(query):\n",
    "    result = search(query)\n",
    "    gen_prompt = prompt(result, query)\n",
    "    answer = run_llm(gen_prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bccfdb1-f3be-4090-a114-715fcbb08db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, you can only get a certificate if you finish the course with a “live” cohort. Certificates are not awarded for the self-paced mode.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('Will I get a certificate?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23804071-e258-4f67-80df-b5de11332332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch client version: (8, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "#To run elastic search\n",
    "from elasticsearch import Elasticsearch\n",
    "import elasticsearch\n",
    "print(f\"Elasticsearch client version: {elasticsearch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d313a73-9c17-42f1-97b5-41ce456ce18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136559/3991661907.py:19: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es_client.indices.create(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "index_settings ={\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "if es_client.indices.exists(index=\"course-questions\"):\n",
    "    es_client.indices.delete(index=\"course-questions\")\n",
    "index_name = \"course-questions\"\n",
    "es_client.indices.create(\n",
    "    index=index_name,\n",
    "    body=index_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f983cb5a-41b9-4edb-b011-baf79e728808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'I just discovered the course. Can I still join?',\n",
       " 'course': 'llm-zoomcamp'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32dfad19-64f1-40c6-b5de-8ed660bd27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67f2ac16-d5ce-4626-9346-4e3e22bc96a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 86/86 [00:00<00:00, 180.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5711c9ad-ae07-4970-8c38-95cbb89befa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": 'Will I get a certificate?',\n",
    "                    \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": \"llm-zoomcamp\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a2da814-c8ae-41a7-924b-b07c851b6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136559/1039276247.py:1: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es_client.search(\n"
     ]
    }
   ],
   "source": [
    "response = es_client.search(\n",
    "    index=index_name,\n",
    "    body=search_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06390742-2ba0-4df3-9096-fb4340d724c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc = []\n",
    "for hit in response['hits']['hits']:\n",
    "    result_doc.append(hit['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "829e8289-74f8-43bd-bc1a-e1a248070d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I missed the first homework - can I still get a certificate?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Issue: I get the notice that due to traffic, I’m on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for “llm zoomcamp”\\nIssue: I’m a pre-existing user from a different zoomcamp and I’m not awarded the free hours even though I’ve submitted my email in the form.\\nAnswer: Just request it via their chat, after you’ve logged in using your pre-existing account, citing “llm zoomcamp” .',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'SaturnCloud - How do I get access?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Using the Openai API does not cost much, you can recharge from 5 dollars. At least for what I spent on the first unit it was barely 5 cents.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'OpenAI: How much will I have to spend to use the Open AI API?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Summer 2025 (via Alexey).',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'When will the course be offered next?',\n",
       "  'course': 'llm-zoomcamp'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc515b4c-2c55-44ed-962d-c6c861b46973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": \"llm-zoomcamp\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    response = es_client.search(\n",
    "    index=index_name,\n",
    "    body=search_query\n",
    "    )\n",
    "    result_doc = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_doc.append(hit['_source'])\n",
    "    return result_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6611c838-e65a-4506-b234-8d2715ec2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    result = elastic_search(query)\n",
    "    gen_prompt = prompt(result, query)\n",
    "    answer = run_llm(gen_prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e05fa209-18ee-4560-bbdb-40970a9db13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136559/3664667401.py:21: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  response = es_client.search(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, you can only get a certificate if you finish the course with a “live” cohort. Certificates are not awarded for the self-paced mode.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('Will I get a certificate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
